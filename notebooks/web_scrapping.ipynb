{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75a26f4",
   "metadata": {},
   "source": [
    "# Dataset Builder \n",
    "To build dataset, pdf articles from following links were taken:\n",
    "- [30 most important articles recommended by Ilya Sutskever](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)\n",
    "- [Habr article with other possible recommendation articles](https://habr.com/ru/companies/ruvds/articles/721150/)\n",
    "- [Deep Learning Papers Reading Roadmap (raw README file)](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebc163",
   "metadata": {},
   "source": [
    "## Web Scrapping of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd3dd09-96ad-4162-8e2d-282fec54142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.envrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3cdd5b-ed3f-46e8-92e9-8abb21d8d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_on_page(page_link):\n",
    "    try:\n",
    "        page = requests.get(page_link)\n",
    "        link = re.findall(r'http.*pdf',page.text)\n",
    "    except:\n",
    "        return 'no link found'\n",
    "    \n",
    "    return link[0] if link else 'no link found'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae62bafd-d98c-4926-ab2a-0d6148a7de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Number of pdfs: 22\n",
      "Overall number: 27\n"
     ]
    }
   ],
   "source": [
    "# idk where did I found this webpage\n",
    "page = requests.get('https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE')\n",
    "print('Status code:', page.status_code)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "links = soup.findAll('a')\n",
    "\n",
    "pdf_list_arc = [link['href'] for link in links if '.pdf' in link['href']] # pdf_files only\n",
    "all_links_arc = [link['href'] for link in links if 'PJLV-iieNCbK-css' in link['class']] # pdf + websites\n",
    "arc_links_info = [{'orig': link, 'pdf': link} for link in pdf_list_arc] # pdf + websites\n",
    "\n",
    "print('Number of pdfs:', len(pdf_list_arc))\n",
    "print('Overall number:', len(all_links_arc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d75eaa1-8821-4d54-8f72-911e9900e85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://habr.com/ru/companies/ruvds/articles/721150/')\n",
    "print('Status code:', page.status_code)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "links = soup.findAll('li')\n",
    "\n",
    "all_links_habr = []\n",
    "for link in links:\n",
    "    for href in link.findAll('a'):\n",
    "        all_links_habr.append(href['href'])\n",
    "all_links_habr = [link for link in all_links_habr if 'habr' not in link and link.startswith('http')]\n",
    "\n",
    "habr_links_info = [] # list to store \"website link\": \"pdf link\" pairs\n",
    "for link in all_links_habr:\n",
    "    if 'pdf' not in link:\n",
    "        habr_links_info.append({\n",
    "            'orig':link,\n",
    "            'pdf': find_pdf_on_page(link)\n",
    "        })\n",
    "    else:\n",
    "        habr_links_info.append({\n",
    "            'orig':link,\n",
    "            'pdf': link\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd85634a-778e-42c7-a25d-899a76968b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = 'https://raw.githubusercontent.com/floodsung/Deep-Learning-Papers-Reading-Roadmap/master/README.md'\n",
    "\n",
    "page = requests.get(page_link)\n",
    "github_links = re.findall(r'http.*pdf',page.text)\n",
    "github_links_info = [{'orig': link, 'pdf': link} for link in github_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed814bb-0dd6-40dd-b50a-fd5d784b4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = habr_links_info + github_links_info + arc_links_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc22b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some arxiv links gave wrong pdf link, which is solved by following code\n",
    "for val in all_links:\n",
    "    if val['pdf'] == \"http://arxiv.org/pdf\":\n",
    "        val['pdf'] = val['orig'].replace('abs', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7f7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check link correction and append only right links\n",
    "links = []\n",
    "for i in all_links:\n",
    "    try:\n",
    "        data = requests.get(i['pdf'])\n",
    "        if data.status_code==200:\n",
    "            links.append(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424fcf40-8c72-47e6-944f-9b9b009f6035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links)\n",
    "df.to_csv('db_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3347d4-5052-4744-ade6-36d6e1fdd5a2",
   "metadata": {},
   "source": [
    "## Dataset building\n",
    "I used PyPDF reader to create a dataset. And stored it as csv file to easy the work of evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8a0c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('db_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34998587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'\\x89PNG\\r'\n",
      "EOF marker not found\n",
      "EOF marker not found\n",
      "EOF marker not found\n",
      "Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "Ignoring wrong pointing object 34 65536 (offset 0)\n",
      "Ignoring wrong pointing object 92 65536 (offset 0)\n",
      "Ignoring wrong pointing object 145 65536 (offset 0)\n",
      "Ignoring wrong pointing object 206 65536 (offset 0)\n",
      "Ignoring wrong pointing object 274 65536 (offset 0)\n",
      "Ignoring wrong pointing object 330 65536 (offset 0)\n",
      "Ignoring wrong pointing object 372 65536 (offset 0)\n",
      "invalid pdf header: b'<!DOC'\n",
      "EOF marker not found\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "for i in range(0, len(df)):\n",
    "    try:\n",
    "        loader = PyPDFLoader(df.loc[i, 'pdf'])\n",
    "        pages.extend(loader.load_and_split())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55be878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55293c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in pages:\n",
    "    texts = text_splitter.split_text(doc.page_content)\n",
    "    for idx, text in enumerate(texts):\n",
    "        docs.append({\n",
    "            'source': doc.metadata['source'],\n",
    "            'page': doc.metadata['page'],\n",
    "            'text': text\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "339019c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(docs)\n",
    "data['id'] = data.index\n",
    "data.head()\n",
    "data.to_csv('article_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf937432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
