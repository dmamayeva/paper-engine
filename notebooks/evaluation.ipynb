{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "For evaluation purpose, Chroma and ElasticSearch were selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To resolve issue with chroma and python 3.12\n",
    "\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='../.envrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "df = pd.read_csv('article_info.csv').drop('Unnamed: 0', axis=1)\n",
    "persist_directory = 'chroma_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataFrameLoader(df, page_content_column='text')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/paper-engine-c3mp4PCs/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:1128\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m   1127\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m-> 1128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/paper-engine-c3mp4PCs/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:1089\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m   1084\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m   1085\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1087\u001b[0m         )\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/paper-engine-c3mp4PCs/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:528\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m texts_with_metadatas \u001b[38;5;241m=\u001b[39m [texts[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    525\u001b[0m embeddings_with_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    526\u001b[0m     [embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids] \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m )\n\u001b[0;32m--> 528\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    531\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    532\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings_with_metadatas,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    533\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts_with_metadatas,\n\u001b[1;32m    534\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids_with_metadata,\n\u001b[1;32m    535\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(documents, embedding_function, persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_1 = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a helpful deep learning mentor. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    \n",
    "    QUESTION: \n",
    "    {question}\n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = format_docs(search_results)\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def search(query):\n",
    "\n",
    "    results = retriever.invoke(query)\n",
    "    return results\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    # print(prompt)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_template = \"\"\"\n",
    "You emulate a student who's taking deep learning class.\n",
    "Formulate 5 questions this student might ask based on a deep learning articles or about deep learning concepts in general. \n",
    "The record should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "Make questions specific to the deep learning details.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "question: {question}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": dataset_template.format(question=prompt)}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What are the fundamental components of a Generative Adversarial Network (GAN), and how do they interact during the training process?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fundamental components of a Generative Adversarial Network (GAN) are the generative model \\( G \\) and the discriminative model \\( D \\). \n",
      "\n",
      "1. **Generative Model (G)**: This model generates new data samples from a prior distribution \\( p_z(z) \\). It maps noise inputs \\( z \\) through a differentiable function \\( G(z; \\theta_g) \\) to produce data-like outputs \\( G(z) \\).\n",
      "\n",
      "2. **Discriminative Model (D)**: This model evaluates the probability that a sample \\( x \\) comes from the actual data distribution \\( p_{\\text{data}}(x) \\) rather than from the generative model \\( p_g(G) \\). It outputs a single scalar, which represents this probability.\n",
      "\n",
      "### Interaction During Training:\n",
      "\n",
      "The training process of GANs involves a minimax game between the generator and the discriminator:\n",
      "\n",
      "- \\( G \\) is trained to minimize the log probability of \\( D \\) classifying its generated samples as fake:\n",
      "  \\[\n",
      "  \\text{min}_G \\log(1 - D(G(z)))\n",
      "  \\]\n",
      "  This means \\( G \\) aims to produce samples that \\( D \\) will mistakenly classify as real.\n",
      "\n",
      "- \\( D \\) is trained to maximize the probability of correctly classifying both real and generated samples:\n",
      "  \\[\n",
      "  \\text{max}_D V(G, D) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
      "  \\]\n",
      "  Here, \\( D \\) learns to distinguish real data from generated data.\n",
      "\n",
      "The training continues iteratively, where \\( D \\) is optimized for several steps (denoted as \\( k \\)) before updating \\( G \\). This alternation allows \\( D \\) to maintain near-optimal performance while \\( G \\) updates its strategy based on the feedback from \\( D \\). The training terminates at a point where the two models reach a balance, ideally resulting in \\( G \\) generating samples indistinguishable from real data, and \\( D \\) being unable to tell the difference (where \\( D(x) = \\frac{1}{2} \\) across the distributions).\n"
     ]
    }
   ],
   "source": [
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topics = [\"Deep Belief Network\", \"ImageNet Evolution\", \"Speech Recognition Evolution\", \"Optimization\", \"Unsupervised Learning / Deep Generative Model\", \n",
    "                  \"RNN\", \"Sequence-to-Sequence Model\", \"Neural Turing Machine\", \"Deep Reinforcement Learning\", \"Deep Transfer Learning\", \"Lifelong Learning\",\n",
    "                  \"One Shot Deep Learning\", \"Natural Language Processing\", \"Object Detection\", \"Visual Tracking\", \"Image Caption\", \"Machine Translation\",\n",
    "                  \"Object Segmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in tqdm(list_of_topics):\n",
    "    questions = generate_questions(topic)\n",
    "    results[topic] = json.loads(questions)['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for topic in results.keys():\n",
    "    for question in results[topic]:\n",
    "        final_results.append({\n",
    "            \"topic\": topic, \n",
    "            \"question\":question['question']\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(final_results).to_csv(\"eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv(\"eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions['id'] = df_questions.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_questions.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth=, lambda q:())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-engine-c3mp4PCs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
